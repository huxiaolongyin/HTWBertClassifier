{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BertForSequenceClassification,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing(Dataset):\n",
    "    \"\"\"\n",
    "    负责对输入的dataframe文本数据进行处理, 返回一个model可训练的dataloader数据\n",
    "    param:\n",
    "        model_path: 选择文本tokenize的模型\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def _preprocess_data(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        text_col: int,\n",
    "        label_col: int,\n",
    "        label_count_least: int,\n",
    "        random_seed: int,\n",
    "    ):\n",
    "        \"\"\"数据初步清洗\"\"\"\n",
    "        # 去重\n",
    "        data = data.drop_duplicates().dropna()\n",
    "        if len(data.columns) >= 2:\n",
    "            # 选择文本和标签列\n",
    "            data[\"text\"] = data.iloc[:, text_col]\n",
    "            data[\"label\"] = data.iloc[:, label_col]\n",
    "            data = data[[\"text\", \"label\"]]\n",
    "\n",
    "            # 增加 label_count_least 条笨笨同学唤醒词数据\n",
    "            repeat_df = pd.DataFrame({\"text\": [\"笨笨同学\"], \"label\": [\"唤醒词\"]})\n",
    "            repeat_df = pd.concat([repeat_df] * label_count_least, ignore_index=True)\n",
    "            data = pd.concat([data, repeat_df], ignore_index=True)\n",
    "\n",
    "            # 选择标签有一定数量的数据\n",
    "            label_value_count = data[\"label\"].value_counts()\n",
    "            label_list = list(\n",
    "                label_value_count[label_value_count > label_count_least].index\n",
    "            )\n",
    "            data = data[data[\"label\"].isin(label_list)]\n",
    "\n",
    "            # 进行标签编码\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            data[\"label\"] = self.label_encoder.fit_transform(data[\"label\"])\n",
    "\n",
    "            # 进行数据均等分\n",
    "            min_label_count = data[\"label\"].value_counts().min()\n",
    "            result_data = pd.DataFrame()\n",
    "            for label in data[\"label\"].unique():\n",
    "                df_sampled = data[data[\"label\"] == label].sample(\n",
    "                    n=min_label_count, random_state=random_seed\n",
    "                )\n",
    "                result_data = pd.concat([result_data, df_sampled])\n",
    "        else:\n",
    "            result_data = data.iloc[:, text_col]\n",
    "\n",
    "        return result_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data[\"text\"][index]\n",
    "        label = self.data[\"label\"][index]\n",
    "        return self._tokenize(text, label)\n",
    "\n",
    "    def _tokenize(self, max_length, data):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            text = list(data[\"text\"].values)\n",
    "            label = list(data[\"label\"].values)\n",
    "        else:\n",
    "            text = list(data)\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained(self.model_path)\n",
    "        tokenized_sentence = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",  # 返回pytorch tensor类型\n",
    "            max_length=max_length,  # 最大长度\n",
    "            padding=\"max_length\",  # 填充长度\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        tokenized_ids = tokenized_sentence[\"input_ids\"]\n",
    "        tokenized_mask = tokenized_sentence[\"attention_mask\"]\n",
    "        tokenized_type_ids = tokenized_sentence[\"token_type_ids\"]\n",
    "\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            return (\n",
    "                tokenized_ids,\n",
    "                tokenized_mask,\n",
    "                tokenized_type_ids,\n",
    "                tensor(label).long(),\n",
    "            )\n",
    "        else:\n",
    "            return tokenized_ids, tokenized_mask, tokenized_type_ids\n",
    "\n",
    "    def _to_dataloader(self, batch_size, max_length, data):\n",
    "        \"\"\"转化为dataloader\"\"\"\n",
    "        tensors = self._tokenize(max_length, data)\n",
    "        dataset = TensorDataset(*tensors)\n",
    "        return DataLoader(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    "        )\n",
    "\n",
    "    def get_dataloader(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        text_loc: int = 0,\n",
    "        label_loc: int = 1,\n",
    "        label_count_least: int = 150,\n",
    "        random_seed: int = 42,\n",
    "        batch_size: int = 5,\n",
    "        test_size: float = 0,\n",
    "        max_length: int = 100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        param:\n",
    "            data: 输入的数据\n",
    "            text_loc: 文本列的位置\n",
    "            label_loc: 标签列的位置\n",
    "            label_count_least: 标签最少数量\n",
    "            random_seed: 随机种子\n",
    "            batch_size: 批次大小\n",
    "            test_size: 测试集比例, 测试集比例不为0, 则返回训练集和测试集\n",
    "            max_length: padding的最大长度\n",
    "        return: dataloader数据\n",
    "        \"\"\"\n",
    "        data = self._preprocess_data(\n",
    "            data, text_loc, label_loc, label_count_least, random_seed\n",
    "        )\n",
    "        if test_size == 0:\n",
    "            return self._to_dataloader(batch_size, max_length, data)\n",
    "        else:\n",
    "            train_data, test_data = train_test_split(\n",
    "                data, test_size=test_size, random_state=random_seed\n",
    "            )\n",
    "            train_dataloder = self._to_dataloader(batch_size, max_length, train_data)\n",
    "            test_dataloder = self._to_dataloader(batch_size, max_length, test_data)\n",
    "            return train_dataloder, test_dataloder\n",
    "\n",
    "    def get_label_count(self):\n",
    "        \"\"\"获取标签数量\"\"\"\n",
    "        return self.label_encoder.classes_.shape[0]\n",
    "\n",
    "    def save_label_encoder(self, path: str = \"output/label_encoder.pkl\"):\n",
    "        \"\"\"保存标签映射文件\"\"\"\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self.label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def flat_accuracy(self, preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "    def train_process(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        num_labels: int,\n",
    "        epochs: int,\n",
    "        train_dataloader: DataLoader,\n",
    "        valid_dataloader: DataLoader,\n",
    "        learning_rate: float = 5e-5,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            model_path,\n",
    "            num_labels=num_labels,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "        )\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss, valid_loss = 0, 0\n",
    "            eval_accuracy = 0\n",
    "\n",
    "            train_pbar = tqdm(\n",
    "                train_dataloader,\n",
    "                total=len(train_dataloader),\n",
    "                desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "            )\n",
    "            for batch in train_pbar:\n",
    "                self.model.zero_grad()\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                labels = batch[3].to(device)\n",
    "                outputs = self.model(\n",
    "                    input_ids, attention_mask=attention_mask, labels=labels\n",
    "                )\n",
    "                loss = outputs[0]\n",
    "                train_loss += loss.item()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            self.model.eval()\n",
    "            valid_pbar = tqdm(\n",
    "                valid_dataloader,\n",
    "                total=len(valid_dataloader),\n",
    "                desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "            )\n",
    "            for batch in valid_pbar:\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                labels = batch[3].to(device)\n",
    "                outputs = self.model(\n",
    "                    input_ids, attention_mask=attention_mask, labels=labels\n",
    "                )\n",
    "                loss = outputs[0]\n",
    "                valid_loss += loss.item()\n",
    "                logits = outputs[1]\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = labels.to(\"cpu\").numpy()\n",
    "                eval_accuracy += self.flat_accuracy(logits, label_ids)\n",
    "\n",
    "            print(f\"Train Epoch: {epoch+1}\")\n",
    "            print(f\"Training Loss: {train_loss/len(train_dataloader):.3f}\")\n",
    "            print(f\"Validation Loss: {valid_loss/len(valid_dataloader):.3f}\")\n",
    "            print(f\"Training Accuracy: {eval_accuracy/len(valid_dataloader):.3f}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model, path)\n",
    "\n",
    "    def predict(self, path, dataloader: DataLoader, device: str = \"cpu\"):\n",
    "        model = torch.load(path, map_location=torch.device(device))\n",
    "        model.eval()\n",
    "        pred_logits = []\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            pred_logits.extend(logits)\n",
    "\n",
    "        final_pred_logits = np.array(pred_logits)\n",
    "        final_pred_logits = np.argmax(final_pred_logits, axis=1)\n",
    "\n",
    "        return final_pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import platform\n",
    "\n",
    "conf = configparser.ConfigParser()\n",
    "conf.read(\"04_aiui_text_cls\\config.ini\", encoding=\"utf-8\")\n",
    "base_path = conf.get(\n",
    "    \"Path\", \"base_window_path\" if platform.system() == \"Windows\" else \"base_linux_path\"\n",
    ")\n",
    "\n",
    "# 数据配置\n",
    "train_data_path = os.path.join(base_path, conf.get(\"DataConfig\", \"train_data_file\"))\n",
    "test_data_path = os.path.join(base_path, conf.get(\"DataConfig\", \"test_data_file\"))\n",
    "text_loc = conf.getint(\"DataConfig\", \"text_loc\")\n",
    "label_loc = conf.getint(\"DataConfig\", \"label_loc\")\n",
    "label_count_least = conf.getint(\"DataConfig\", \"label_count_least\")\n",
    "random_seed = conf.getint(\"DataConfig\", \"random_seed\")\n",
    "test_size = conf.getfloat(\"DataConfig\", \"test_size\")\n",
    "max_length = conf.getint(\"DataConfig\", \"max_length\")\n",
    "batch_size = conf.getint(\"DataConfig\", \"batch_size\")\n",
    "label_encode_path = os.path.join(base_path, conf.get(\"DataConfig\", \"label_encode_path\"))\n",
    "\n",
    "# 模型配置\n",
    "bert_model_path = os.path.join(base_path, conf.get(\"ModelConfig\", \"bert_model_path\"))\n",
    "epochs = conf.getint(\"ModelConfig\", \"epochs\")\n",
    "learning_rate = conf.getfloat(\"ModelConfig\", \"learning_rate\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "output_model_path = os.path.join(\n",
    "    base_path, conf.get(\"ModelConfig\", \"output_model_path\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 数据处理\n",
    "train_data = pd.read_csv(train_data_path, sep=\";\", header=0)\n",
    "DP = DataProcessing(bert_model_path)\n",
    "train_dataloder, valid_dataloader = DP.get_dataloader(\n",
    "    train_data,\n",
    "    text_loc,\n",
    "    label_loc,\n",
    "    label_count_least,\n",
    "    random_seed,\n",
    "    batch_size,\n",
    "    test_size,\n",
    "    max_length,\n",
    ")\n",
    "\n",
    "# 保存标签映射\n",
    "DP.save_label_encoder(label_encode_path)\n",
    "\n",
    "# 模型训练\n",
    "num_labels = DP.get_label_count()\n",
    "classify_model = ClassifyModel()\n",
    "classify_model.train_process(\n",
    "    bert_model_path,\n",
    "    num_labels,\n",
    "    epochs,\n",
    "    train_dataloder,\n",
    "    valid_dataloader,\n",
    "    learning_rate,\n",
    "    device,\n",
    ")\n",
    "\n",
    "# 保存模型和tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_path)\n",
    "classify_model.model.save_pretrained(output_model_path)\n",
    "tokenizer.save_pretrained(output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TextCls\n",
    "\n",
    "TC = TextCls()\n",
    "data = pd.read_csv(test_data_path, sep=\";\", header=0)\n",
    "data[\"predict\"] = TC.htw_text_cls(list(data[\"text\"]))\n",
    "result = (\n",
    "    data.groupby([\"label\"]).agg({\"text\": \"count\", \"is_correct\": \"sum\"}).reset_index()\n",
    ")\n",
    "result[\"correct_rate\"] = round(result[\"is_correct\"] / result[\"text\"] * 100, 2)\n",
    "print(\n",
    "    \"avg_accucy: \", round(data[\"is_correct\"].sum() / data[\"text\"].count() * 100, 2), \"%\"\n",
    ")\n",
    "result.sort_values(by=\"correct_rate\", ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
